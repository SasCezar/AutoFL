{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoFL","text":"<p>Automatic source code file annotation using weak labeling.</p>"},{"location":"#overview","title":"Overview","text":"<p>AutoFL is a tool designed for automatic annotation of source code files through weak labeling techniques. It provides both an API and a web-based UI for easy analysis of projects across different languages.</p>"},{"location":"#setup","title":"Setup","text":"<p>To set up the repository along with its UI submodule, clone it using:</p> <pre><code>git clone --recursive git@github.com:SasCezar/AutoFL.git AutoFL\n</code></pre>"},{"location":"#optional-model-setup","title":"Optional Model Setup","text":"<p>For advanced features like semantic-based labeling, download models as required. For example, to use w2v-so, download the model from here and place it in the <code>data/models/w2v-so</code> folder. Alternatively, you can provide a custom path in the configuration files.</p>"},{"location":"#usage","title":"Usage","text":"<p>To run the tool using Docker, navigate to the project directory (where the <code>docker-compose.yaml</code> file is located) and execute:</p> <pre><code>docker compose up\n</code></pre>"},{"location":"#api-endpoint","title":"API Endpoint","text":"<p>To analyze the files of a project, make a POST request to the following endpoint:</p> <pre><code>curl -X POST -d '{\"name\": \"&lt;PROJECT_NAME&gt;\", \"remote\": \"&lt;PROJECT_REMOTE&gt;\", \"languages\": [\"&lt;PROGRAMMING_LANGUAGE&gt;\"]}' localhost:8000/label/files -H \"content-type: application/json\"\n</code></pre> <p>For instance, to analyze the project at https://github.com/mickleness/pumpernickel, use:</p> <pre><code>curl -X POST -d '{\"name\": \"pumpernickel\", \"remote\": \"https://github.com/mickleness/pumpernickel\", \"languages\": [\"java\"]}' localhost:8000/label/files -H \"content-type: application/json\"\n</code></pre>"},{"location":"#web-ui","title":"Web UI","text":"<p>AutoFL provides a web-based UI accessible locally at http://localhost:8501:</p> <p></p> <p>For more details, check the UI repository.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>AutoFL uses Hydra to manage configurations. The configuration files can be found in the <code>config</code> folder. The main configuration file, <code>main.yaml</code>, allows you to customize various options:</p> <ul> <li>local: Choose between local or Docker environments. Docker is the default.</li> <li>taxonomy: Set the taxonomy for labeling. Currently supports gitranking. You can add custom taxonomies.</li> <li>annotator: Specify the annotators to use. The default is simple, offering good results without dependencies on language models.</li> <li>version_strategy: Select the versioning strategy. The default is latest.</li> <li>dataloader: Choose the dataloader. The default is postgres.</li> <li>writer: Set the writer for storing results. The default is postgres.</li> </ul> <p>Additional configurations can be added by creating new files in the corresponding component folders.</p>"},{"location":"#functionalities","title":"Functionalities","text":"<ul> <li>Annotation (UI/API/Script)</li> <li>File-Level</li> <li>Package-Level</li> <li>Project-Level</li> <li>Batch Analysis (Script Only)</li> <li>Temporal Analysis (TODO)</li> <li>Classification (TODO)</li> </ul>"},{"location":"#supported-languages","title":"Supported Languages","text":"<ul> <li>Java</li> <li>Python (untested)</li> <li>C (untested)</li> <li>C++ (untested)</li> <li>C# (untested)</li> </ul>"},{"location":"#development","title":"Development","text":"<p>AutoFL is composed of multiple components, as shown in the architecture diagram below:</p> <p></p>"},{"location":"#adding-support-for-new-languages","title":"Adding Support for New Languages","text":"<p>To add support for additional languages, a language-specific parser is required. You can use tree-sitter to develop a parser quickly.</p>"},{"location":"#parser-details","title":"Parser Details","text":"<p>The parser needs to be located in the <code>parser/languages</code> folder. It should extend the <code>BaseParser</code> class, which follows this structure:</p> <pre><code>class ParserBase(ABC):\n\"\"\"\nAbstract class for a programming language parser.\n\"\"\"\n\n    def __init__(self, library_path: Path | str):\n        \"\"\"\n        :param library_path: Path to the tree-sitter languages.so file. The file has to contain the\n        language parser. See tree-sitter for more details\n        \"\"\"\n        ...\n</code></pre> <p>To implement the parsing logic, create a class that handles extracting identifiers. For Python, the parser might look like:</p> <pre><code>class PythonParser(ParserBase, lang=Extension.python.name):\n    \"\"\"\n    Python-specific parser using a generic grammar for multiple versions. Utilizes tree-sitter for AST extraction.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        ...\n</code></pre> <p>A custom parser independent of tree-sitter can also be developed. For more details, refer to the implementation of ParserBase.</p>"},{"location":"#known-issues","title":"Known Issues","text":"<ul> <li>Dependency Installation: The setup process may take significant time (~10 minutes), and dependency installations might fail due to timeouts. This appears to be a network-related issue, and retrying often resolves it. Future updates will aim to simplify dependencies.</li> <li>~~Indefinite Analysis Loops~~: ~~In some projects, the analysis may loop indefinitely. This issue is currently under investigation.~~ Seems solved in the latest version. Will monitor for further occurrences.</li> </ul>"},{"location":"#docker-image-availability","title":"Docker Image Availability","text":"<p>AutoFL is also available as a Docker image. You can pull the image from Docker Hub using:</p> <pre><code>docker pull cezarsas/autofl\n</code></pre> <p>Find more details and updates at the Docker Hub page.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This tool is in active development and may not function as expected in some cases. It has been tested primarily on Docker versions <code>24.0.7</code> and <code>25.0.0</code> for <code>Ubuntu 22.04</code>. Limited testing has been performed on <code>Windows</code> and <code>MacOS</code>, where functionality may vary.</p> <p>If you encounter any issues, please open an issue on GitHub, make a pull request, or contact me at <code>c.a.sas@rug.nl</code>.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this tool useful, please cite our work:</p>"},{"location":"#paper","title":"Paper","text":"<pre><code>@article{sas2024multigranular,\ntitle     = {Multi-granular Software Annotation using File-level Weak Labelling},\nauthor    = {Cezar Sas and Andrea Capiluppi},\njournal   = {Empirical Software Engineering},\nvolume    = {29},\nnumber    = {1},\npages     = {12},\nyear      = {2024},\nurl       = {https://doi.org/10.1007/s10664-023-10423-7},\ndoi       = {10.1007/s10664-023-10423-7}\n}\n</code></pre> <p>Note: The code used in this paper is available at CodeGraphClassification. However, AutoFL provides enhanced features, is more user-friendly, and includes a UI.</p>"},{"location":"#tool","title":"Tool","text":"<pre><code>@software{sas2023autofl,\n          author    = {Sas, Cezar and Capiluppi, Andrea},\n          month     = oct,\n          title     = {{AutoFL}},\n          url       = {https://github.com/SasCezar/AutoFL},\n          version   = {0.5.0},\n          year      = {2024},\n          url       = {https://doi.org/10.5281/zenodo.10255368},\n          doi       = {10.5281/zenodo.10255368}\n}\n</code></pre>"},{"location":"reference/SUMMARY/","title":"Code Reference","text":"<ul> <li>annotation<ul> <li>annotator</li> <li>filtering</li> <li>keyword</li> <li>lf</li> <li>similarity</li> <li>transformation</li> </ul> </li> <li>api<ul> <li>main</li> <li>run_analysis</li> </ul> </li> <li>batch<ul> <li>batch_annotation</li> <li>batch_keyword_extraction</li> </ul> </li> <li>dataloader<ul> <li>csv_dataloader</li> <li>dataloader</li> <li>gitranking</li> <li>json_dataloader</li> <li>postgres_dataloader</li> </ul> </li> <li>embedding<ul> <li>abstract</li> <li>ft</li> <li>gensim_w2v</li> <li>huggingface</li> <li>spacy_bert</li> </ul> </li> <li>ensemble<ul> <li>avg</li> <li>cascade</li> <li>ensemble</li> <li>none</li> <li>voting</li> </ul> </li> <li>entity<ul> <li>analysis</li> <li>annotation</li> <li>file</li> <li>project</li> <li>taxonomy</li> </ul> </li> <li>execution<ul> <li>annotation</li> <li>execution</li> <li>keyword_extraction</li> </ul> </li> <li>keyword_extraction<ul> <li>keyword_extraction</li> <li>rake</li> <li>yake</li> </ul> </li> <li>parser<ul> <li>extensions</li> <li>languages<ul> <li>c</li> <li>cpp</li> <li>csharp</li> <li>java</li> <li>python</li> </ul> </li> <li>parser</li> </ul> </li> <li>pipeline<ul> <li>file_annotation</li> <li>identifier_extraction</li> <li>keyword_extraction</li> <li>package_annotation</li> <li>pipeline</li> <li>project_annotation</li> </ul> </li> <li>utils<ul> <li>instantiators</li> <li>utils</li> </ul> </li> <li>vcs<ul> <li>current</li> <li>date_range</li> <li>first</li> <li>latest</li> <li>vcs</li> <li>version_strategy</li> </ul> </li> <li>writer<ul> <li>file</li> <li>keyword_sql</li> <li>postgres</li> <li>writer</li> </ul> </li> </ul>"},{"location":"reference/annotation/","title":"Index","text":""},{"location":"reference/annotation/annotator/","title":"Annotator","text":""},{"location":"reference/annotation/annotator/#annotation.annotator.Annotator","title":"<code>Annotator</code>","text":"<p>Class that performs the annotation of a file.</p> Source code in <code>src/annotation/annotator.py</code> <pre><code>class Annotator:\n    \"\"\"\n    Class that performs the annotation of a file.\n    \"\"\"\n    def __init__(self, lf: LFBase, filtering: FilteringBase,\n                 transformation: TransformationBase, name: str = None):\n        \"\"\"\n\n        :param lf: Labelling function to be used to annotate the files.\n        :param filtering: Filtering strategy to be used to mark as unannotated files with noisy annotations.\n        :param transformation: Transformation to apply to the annotation vector.\n        \"\"\"\n        self.lf = lf\n        self.filtering = filtering\n        self.transformation = transformation\n        self.name = name\n\n    def annotate(self, name: str, content: str) -&gt; Annotation:\n\n        label_vec = self.lf.annotate(name, content)\n\n        unannotated = False\n        if self.filtering:\n            unannotated = self.filtering.filter(label_vec)\n\n        raw_vec = label_vec.copy()\n        if self.transformation and not unannotated:\n            label_vec = self.transformation.transform(label_vec)\n\n        if not np.linalg.norm(label_vec):\n            unannotated = 1\n\n        annotation = Annotation(distribution=list(label_vec), unannotated=unannotated, raw_annotation=raw_vec)\n        return annotation\n</code></pre>"},{"location":"reference/annotation/annotator/#annotation.annotator.Annotator.__init__","title":"<code>__init__(lf, filtering, transformation, name=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LFBase</code> <p>Labelling function to be used to annotate the files.</p> required <code>filtering</code> <code>FilteringBase</code> <p>Filtering strategy to be used to mark as unannotated files with noisy annotations.</p> required <code>transformation</code> <code>TransformationBase</code> <p>Transformation to apply to the annotation vector.</p> required Source code in <code>src/annotation/annotator.py</code> <pre><code>def __init__(self, lf: LFBase, filtering: FilteringBase,\n             transformation: TransformationBase, name: str = None):\n    \"\"\"\n\n    :param lf: Labelling function to be used to annotate the files.\n    :param filtering: Filtering strategy to be used to mark as unannotated files with noisy annotations.\n    :param transformation: Transformation to apply to the annotation vector.\n    \"\"\"\n    self.lf = lf\n    self.filtering = filtering\n    self.transformation = transformation\n    self.name = name\n</code></pre>"},{"location":"reference/annotation/filtering/","title":"Filtering","text":""},{"location":"reference/annotation/filtering/#annotation.filtering.FilteringBase","title":"<code>FilteringBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for annotation filtering.</p> Source code in <code>src/annotation/filtering.py</code> <pre><code>class FilteringBase(ABC):\n    \"\"\"\n    Base class for annotation filtering.\n    \"\"\"\n\n    @abstractmethod\n    def filter(self, distribution: np.array) -&gt; bool:\n        \"\"\"\n        Given a numpy array representing a distribution, return a boolean on whether the annotation\n        is filtered or not.\n        :param distribution:\n        :return:\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/annotation/filtering/#annotation.filtering.FilteringBase.filter","title":"<code>filter(distribution)</code>  <code>abstractmethod</code>","text":"<p>Given a numpy array representing a distribution, return a boolean on whether the annotation is filtered or not.</p> <p>Parameters:</p> Name Type Description Default <code>distribution</code> <code>array</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>src/annotation/filtering.py</code> <pre><code>@abstractmethod\ndef filter(self, distribution: np.array) -&gt; bool:\n    \"\"\"\n    Given a numpy array representing a distribution, return a boolean on whether the annotation\n    is filtered or not.\n    :param distribution:\n    :return:\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/annotation/filtering/#annotation.filtering.JSDFiltering","title":"<code>JSDFiltering</code>","text":"<p>               Bases: <code>FilteringBase</code></p> <p>Filters the annotation using the JSD between it and a uniform distribution. If the score is lower than a threshold (noisy annotation), the annotation is filtered.</p> Source code in <code>src/annotation/filtering.py</code> <pre><code>class JSDFiltering(FilteringBase):\n    \"\"\"\n    Filters the annotation using the JSD between it and a uniform distribution.\n    If the score is lower than a threshold (noisy annotation), the annotation is filtered.\n    \"\"\"\n\n    def __init__(self, threshold: float):\n        \"\"\"\n\n        :param threshold: JSD threshold used to filter the file\n        \"\"\"\n        self.threshold = threshold\n\n    def filter(self, distribution: np.array) -&gt; bool:\n        n = len(distribution)\n        uniform_vec = np.ones(n) / n\n\n        if (\n            np.linalg.norm(distribution) == 0\n            or jensenshannon(distribution, uniform_vec) &lt;= self.threshold\n        ):\n            return True\n\n        return False\n</code></pre>"},{"location":"reference/annotation/filtering/#annotation.filtering.JSDFiltering.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>JSD threshold used to filter the file</p> required Source code in <code>src/annotation/filtering.py</code> <pre><code>def __init__(self, threshold: float):\n    \"\"\"\n\n    :param threshold: JSD threshold used to filter the file\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/annotation/filtering/#annotation.filtering.ThresholdFiltering","title":"<code>ThresholdFiltering</code>","text":"<p>               Bases: <code>FilteringBase</code></p> <p>Filter based on the probability of the most likely class. If is not high enough, then the annotation is filtered.</p> Source code in <code>src/annotation/filtering.py</code> <pre><code>class ThresholdFiltering(FilteringBase):\n    \"\"\"\n    Filter based on the probability of the most likely class.\n    If is not high enough, then the annotation is filtered.\n    \"\"\"\n\n    def __init__(self, threshold: float):\n        \"\"\"\n        :param threshold: Noise threshold used to filter the file\n        \"\"\"\n        self.threshold = threshold\n\n    def filter(self, distribution: np.array) -&gt; bool:\n        if np.linalg.norm(distribution) == 0 or np.max(distribution) &lt;= self.threshold:\n            return False\n\n        return True\n</code></pre>"},{"location":"reference/annotation/filtering/#annotation.filtering.ThresholdFiltering.__init__","title":"<code>__init__(threshold)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Noise threshold used to filter the file</p> required Source code in <code>src/annotation/filtering.py</code> <pre><code>def __init__(self, threshold: float):\n    \"\"\"\n    :param threshold: Noise threshold used to filter the file\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"reference/annotation/keyword/","title":"Keyword","text":""},{"location":"reference/annotation/keyword/#annotation.keyword.KeywordLF","title":"<code>KeywordLF</code>","text":"<p>               Bases: <code>LFBase</code></p> <p>Labelling function that uses identifiers and keywords.</p> Source code in <code>src/annotation/keyword.py</code> <pre><code>class KeywordLF(LFBase):\n    \"\"\"\n    Labelling function that uses identifiers and keywords.\n    \"\"\"\n\n    def __init__(self, taxonomy: TaxonomyBase, key: str):\n        super().__init__(taxonomy)\n        self.key = key\n\n    def annotate(self, name: str, content: str) -&gt; np.array:\n        \"\"\"\n        Compute the probability for the file given the name and/or the content.\n\n        :param name: Source file name\n        :param content: Content of the file (usually identifiers)\n        :return:\n        \"\"\"\n        node_labels = np.zeros(len(self.taxonomy))\n        for _label in self.taxonomy:\n            label: KeywordLabel = _label\n            intersection = list(label.keywords.intersection(Multiset(content.split())))\n            intersection = Counter(intersection)\n            node_labels[label.index] = sum(\n                [intersection[k] * label.weights[k] for k in intersection.keys()]\n            )\n\n        norm = np.sum(node_labels)\n        node_vec = node_labels / norm if norm &gt; 0 else np.zeros(len(self.taxonomy))\n\n        return node_vec\n</code></pre>"},{"location":"reference/annotation/keyword/#annotation.keyword.KeywordLF.annotate","title":"<code>annotate(name, content)</code>","text":"<p>Compute the probability for the file given the name and/or the content.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Source file name</p> required <code>content</code> <code>str</code> <p>Content of the file (usually identifiers)</p> required <p>Returns:</p> Type Description <code>array</code> Source code in <code>src/annotation/keyword.py</code> <pre><code>def annotate(self, name: str, content: str) -&gt; np.array:\n    \"\"\"\n    Compute the probability for the file given the name and/or the content.\n\n    :param name: Source file name\n    :param content: Content of the file (usually identifiers)\n    :return:\n    \"\"\"\n    node_labels = np.zeros(len(self.taxonomy))\n    for _label in self.taxonomy:\n        label: KeywordLabel = _label\n        intersection = list(label.keywords.intersection(Multiset(content.split())))\n        intersection = Counter(intersection)\n        node_labels[label.index] = sum(\n            [intersection[k] * label.weights[k] for k in intersection.keys()]\n        )\n\n    norm = np.sum(node_labels)\n    node_vec = node_labels / norm if norm &gt; 0 else np.zeros(len(self.taxonomy))\n\n    return node_vec\n</code></pre>"},{"location":"reference/annotation/lf/","title":"Lf","text":""},{"location":"reference/annotation/lf/#annotation.lf.LFBase","title":"<code>LFBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract labelling function. Uses to annotate</p> Source code in <code>src/annotation/lf.py</code> <pre><code>class LFBase(ABC):\n    \"\"\"\n    Abstract labelling function. Uses to annotate\n    \"\"\"\n\n    def __init__(self, taxonomy: TaxonomyBase):\n        \"\"\"\n        Taxonomy containing labels to annotate the examples with.\n        :param taxonomy:\n        \"\"\"\n        self.taxonomy = taxonomy\n        self.content = True\n\n    def annotate(self, name: str, content: str) -&gt; np.array:\n        \"\"\"\n        Annotate an example given the file name and it's content.\n        :param name: Source file name\n        :param content: Content of the file (usually identifiers)\n        :return:\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/annotation/lf/#annotation.lf.LFBase.__init__","title":"<code>__init__(taxonomy)</code>","text":"<p>Taxonomy containing labels to annotate the examples with.</p> <p>Parameters:</p> Name Type Description Default <code>taxonomy</code> <code>TaxonomyBase</code> required Source code in <code>src/annotation/lf.py</code> <pre><code>def __init__(self, taxonomy: TaxonomyBase):\n    \"\"\"\n    Taxonomy containing labels to annotate the examples with.\n    :param taxonomy:\n    \"\"\"\n    self.taxonomy = taxonomy\n    self.content = True\n</code></pre>"},{"location":"reference/annotation/lf/#annotation.lf.LFBase.annotate","title":"<code>annotate(name, content)</code>","text":"<p>Annotate an example given the file name and it's content.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Source file name</p> required <code>content</code> <code>str</code> <p>Content of the file (usually identifiers)</p> required <p>Returns:</p> Type Description <code>array</code> Source code in <code>src/annotation/lf.py</code> <pre><code>def annotate(self, name: str, content: str) -&gt; np.array:\n    \"\"\"\n    Annotate an example given the file name and it's content.\n    :param name: Source file name\n    :param content: Content of the file (usually identifiers)\n    :return:\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/annotation/similarity/","title":"Similarity","text":""},{"location":"reference/annotation/similarity/#annotation.similarity.SimilarityLF","title":"<code>SimilarityLF</code>","text":"<p>               Bases: <code>LFBase</code></p> <p>Labelling function that uses vector similarity to label examples.</p> Source code in <code>src/annotation/similarity.py</code> <pre><code>class SimilarityLF(LFBase):\n    \"\"\"\n    Labelling function that uses vector similarity to label examples.\n    \"\"\"\n\n    def __init__(self, taxonomy: TaxonomyBase, embedding: AbstractEmbeddingModel):\n        super().__init__(taxonomy)\n        self.embedding = embedding\n        self.label_vecs = self.embed_labels()\n        self.content = False\n\n    def annotate(self, name: str, content: str) -&gt; np.array:\n        content_vec = [self.embedding.get_embedding(content.lower())]\n\n        try:\n            sims = cosine_similarity(content_vec, self.label_vecs)\n        except ValueError:\n            logger.error(\n                f\"Error in {name}\\nContent: {content}\\nContent vec: {content_vec}\"\n            )\n            sims = np.zeros(len(self.label_vecs)) - 1\n\n        # Adding 1 (-1 is the lowest value for cosine sim) to bring the vector in the range 0-1 when normalizing\n        sims = sims[0] + 1\n        norm = np.linalg.norm(sims)\n        node_labels = sims / norm if norm else sims\n\n        return node_labels\n\n    def embed_labels(self) -&gt; List[np.array]:\n        res = []\n        for label in self.taxonomy:\n            res.append(self.embedding.get_embedding(label.name.lower()))\n\n        return res\n</code></pre>"},{"location":"reference/annotation/transformation/","title":"Transformation","text":""},{"location":"reference/annotation/transformation/#annotation.transformation.TopLabels","title":"<code>TopLabels</code>","text":"<p>               Bases: <code>TransformationBase</code></p> <p>Picks the probabilities with a threshold higher than min_threshold</p> Source code in <code>src/annotation/transformation.py</code> <pre><code>class TopLabels(TransformationBase):\n    \"\"\"\n    Picks the probabilities with a threshold higher than min_threshold\n    \"\"\"\n\n    def __init__(self, min_threshold: float = 0.05):\n        self.min_threshold = min_threshold\n\n    def transform(self, distribution: np.array) -&gt; np.array:\n        res_distribution = distribution &gt; self.min_threshold\n\n        norm = np.linalg.norm(res_distribution)\n        if norm != 0:\n            res_distribution = res_distribution / norm\n        return res_distribution\n</code></pre>"},{"location":"reference/api/","title":"Index","text":""},{"location":"reference/api/main/","title":"Main","text":""},{"location":"reference/api/run_analysis/","title":"Run analysis","text":""},{"location":"reference/batch/","title":"Index","text":""},{"location":"reference/batch/batch_annotation/","title":"Batch annotation","text":""},{"location":"reference/batch/batch_keyword_extraction/","title":"Batch keyword extraction","text":""},{"location":"reference/dataloader/","title":"Index","text":""},{"location":"reference/dataloader/csv_dataloader/","title":"Csv dataloader","text":""},{"location":"reference/dataloader/dataloader/","title":"Dataloader","text":""},{"location":"reference/dataloader/gitranking/","title":"Gitranking","text":""},{"location":"reference/dataloader/json_dataloader/","title":"Json dataloader","text":""},{"location":"reference/dataloader/postgres_dataloader/","title":"Postgres dataloader","text":""},{"location":"reference/dataloader/postgres_dataloader/#dataloader.postgres_dataloader.PostgresProjectLoader","title":"<code>PostgresProjectLoader</code>","text":"<p>               Bases: <code>DataLoaderBase</code></p> Source code in <code>src/dataloader/postgres_dataloader.py</code> <pre><code>class PostgresProjectLoader(DataLoaderBase):\n    def __init__(self, host, db, user, password):\n        super().__init__()\n        self.host = host\n        self.db = db\n        self.user = user\n        self.password = password\n        self.engine = sqlalchemy.create_engine(\n            f\"postgresql+psycopg://{user}:{password}@{host}/{db}\"\n        )\n        self.metadata = sqlalchemy.MetaData()\n        self.projects = sqlalchemy.Table(\n            \"project\", self.metadata, autoload_with=self.engine\n        )\n\n    def load(self, projects_list: list[str] = None) -&gt; Iterable[Project]:\n        \"\"\"\n        Executes sql alchemy query to load all projects from the database that are in the projects_list\n        :param cfg:\n        :param projects_list:\n        :return:\n        \"\"\"\n        if projects_list is not None:\n            projects_list = {int(x) for x in projects_list}\n            query = sqlalchemy.select(self.projects).where(\n                (self.projects.c.id.in_(projects_list))\n            )\n        else:\n            query = sqlalchemy.select(self.projects)\n        with self.engine.connect() as conn:\n\n            result = conn.execute(query)\n            for row in result:\n                proj = Project(**json.loads(row[5]))\n                proj.versions = [Version(**json.loads(row[6]))]\n                yield proj\n\n    def find_projects(self, cfg: dict = None):\n        \"\"\"\n        Executes sql alchemy query to load all projects from the database\n        :return:\n        \"\"\"\n        with self.engine.connect() as conn:\n            if cfg is None:\n                query = sqlalchemy.select(self.projects.c.id)\n            else:\n                query = sqlalchemy.select(self.projects.c.id).where(\n                    self.projects.c.config == cfg\n                )\n            result = conn.execute(query)\n            for row in result:\n                yield row[0]\n\n    def load_single(self, project: str, cfg: dict):\n        \"\"\"\n        Executes sql alchemy query to load a single project and all versions from the database\n        :param project:\n        :param cfg:\n        :return:\n        \"\"\"\n        with self.engine.connect() as conn:\n            query = sqlalchemy.select(self.projects).where(\n                (self.projects.c.name == project) &amp; (self.projects.c.config == cfg)\n            )\n            result = list(conn.execute(query))\n            project = Project(**json.loads(result[0][5]))\n            project.versions = []\n            for row in result:\n                project.versions.append(Version(**json.loads(row[6])))\n\n            return project\n</code></pre>"},{"location":"reference/dataloader/postgres_dataloader/#dataloader.postgres_dataloader.PostgresProjectLoader.find_projects","title":"<code>find_projects(cfg=None)</code>","text":"<p>Executes sql alchemy query to load all projects from the database</p> <p>Returns:</p> Type Description Source code in <code>src/dataloader/postgres_dataloader.py</code> <pre><code>def find_projects(self, cfg: dict = None):\n    \"\"\"\n    Executes sql alchemy query to load all projects from the database\n    :return:\n    \"\"\"\n    with self.engine.connect() as conn:\n        if cfg is None:\n            query = sqlalchemy.select(self.projects.c.id)\n        else:\n            query = sqlalchemy.select(self.projects.c.id).where(\n                self.projects.c.config == cfg\n            )\n        result = conn.execute(query)\n        for row in result:\n            yield row[0]\n</code></pre>"},{"location":"reference/dataloader/postgres_dataloader/#dataloader.postgres_dataloader.PostgresProjectLoader.load","title":"<code>load(projects_list=None)</code>","text":"<p>Executes sql alchemy query to load all projects from the database that are in the projects_list</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> required <code>projects_list</code> <code>list[str]</code> <code>None</code> <p>Returns:</p> Type Description <code>Iterable[Project]</code> Source code in <code>src/dataloader/postgres_dataloader.py</code> <pre><code>def load(self, projects_list: list[str] = None) -&gt; Iterable[Project]:\n    \"\"\"\n    Executes sql alchemy query to load all projects from the database that are in the projects_list\n    :param cfg:\n    :param projects_list:\n    :return:\n    \"\"\"\n    if projects_list is not None:\n        projects_list = {int(x) for x in projects_list}\n        query = sqlalchemy.select(self.projects).where(\n            (self.projects.c.id.in_(projects_list))\n        )\n    else:\n        query = sqlalchemy.select(self.projects)\n    with self.engine.connect() as conn:\n\n        result = conn.execute(query)\n        for row in result:\n            proj = Project(**json.loads(row[5]))\n            proj.versions = [Version(**json.loads(row[6]))]\n            yield proj\n</code></pre>"},{"location":"reference/dataloader/postgres_dataloader/#dataloader.postgres_dataloader.PostgresProjectLoader.load_single","title":"<code>load_single(project, cfg)</code>","text":"<p>Executes sql alchemy query to load a single project and all versions from the database</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> required <code>cfg</code> <code>dict</code> required <p>Returns:</p> Type Description Source code in <code>src/dataloader/postgres_dataloader.py</code> <pre><code>def load_single(self, project: str, cfg: dict):\n    \"\"\"\n    Executes sql alchemy query to load a single project and all versions from the database\n    :param project:\n    :param cfg:\n    :return:\n    \"\"\"\n    with self.engine.connect() as conn:\n        query = sqlalchemy.select(self.projects).where(\n            (self.projects.c.name == project) &amp; (self.projects.c.config == cfg)\n        )\n        result = list(conn.execute(query))\n        project = Project(**json.loads(result[0][5]))\n        project.versions = []\n        for row in result:\n            project.versions.append(Version(**json.loads(row[6])))\n\n        return project\n</code></pre>"},{"location":"reference/embedding/","title":"Index","text":""},{"location":"reference/embedding/abstract/","title":"Abstract","text":""},{"location":"reference/embedding/abstract/#embedding.abstract.AbstractEmbeddingModel","title":"<code>AbstractEmbeddingModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for embedding models.</p> Source code in <code>src/embedding/abstract.py</code> <pre><code>class AbstractEmbeddingModel(ABC):\n    \"\"\"\n    Abstract class for embedding models.\n    \"\"\"\n\n    def __init__(self, split_camel: bool = False):\n        self._name = \"AbstractEmbeddingModel\"\n        self.model = None\n        self._split_camel = split_camel\n\n    @abstractmethod\n    def get_embedding(self, text: str) -&gt; numpy.ndarray:\n        pass\n\n    def split(self, name: str):\n        if self._split_camel:\n            return split_camelcase(name)\n        return name.split(\" \")\n</code></pre>"},{"location":"reference/embedding/ft/","title":"Ft","text":""},{"location":"reference/embedding/ft/#embedding.ft.FastTextEmbedding","title":"<code>FastTextEmbedding</code>","text":"<p>               Bases: <code>AbstractEmbeddingModel</code></p> <p>Class for embedding models using FastText model.</p> Source code in <code>src/embedding/ft.py</code> <pre><code>class FastTextEmbedding(AbstractEmbeddingModel):\n    \"\"\"\n    Class for embedding models using FastText model.\n    \"\"\"\n\n    def __init__(self, path: str, model: str = \"fastText\", split_camel: bool = False):\n        super().__init__(split_camel=split_camel)\n        self._name = f\"{model}\"\n        self.model = ft.load_model(path)\n\n    def get_embedding(self, text: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the embedding of the text.\n        :param text:\n        :return:\n        \"\"\"\n        if self._split_camel:\n            text = \" \".join(self.split(text))\n        return self.model.get_sentence_vector(text)\n</code></pre>"},{"location":"reference/embedding/ft/#embedding.ft.FastTextEmbedding.get_embedding","title":"<code>get_embedding(text)</code>","text":"<p>Returns the embedding of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/embedding/ft.py</code> <pre><code>def get_embedding(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the embedding of the text.\n    :param text:\n    :return:\n    \"\"\"\n    if self._split_camel:\n        text = \" \".join(self.split(text))\n    return self.model.get_sentence_vector(text)\n</code></pre>"},{"location":"reference/embedding/gensim_w2v/","title":"Gensim w2v","text":""},{"location":"reference/embedding/gensim_w2v/#embedding.gensim_w2v.W2VEmbedding","title":"<code>W2VEmbedding</code>","text":"<p>               Bases: <code>AbstractEmbeddingModel</code></p> <p>Class for embedding models using Word2Vec model.</p> Source code in <code>src/embedding/gensim_w2v.py</code> <pre><code>class W2VEmbedding(AbstractEmbeddingModel):\n    \"\"\"\n    Class for embedding models using Word2Vec model.\n    \"\"\"\n\n    def __init__(self, path: str, model: str = \"W2V-Unk\", split_camel: bool = False):\n        super().__init__(split_camel)\n        self._name = f\"{model}\"\n        self.model = KeyedVectors.load_word2vec_format(path, binary=True)\n\n    def get_embedding(self, text: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the embedding of the text.\n        :param text:\n        :return:\n        \"\"\"\n        embeddings = []\n        if not text:\n            embeddings.append(np.zeros(self.model.vector_size))\n        for word in self.split(str(text)):\n            if word in self.model:\n                embeddings.append(self.model[word])\n            else:\n                embeddings.append(np.zeros(self.model.vector_size))\n        return np.mean(embeddings, axis=0)\n</code></pre>"},{"location":"reference/embedding/gensim_w2v/#embedding.gensim_w2v.W2VEmbedding.get_embedding","title":"<code>get_embedding(text)</code>","text":"<p>Returns the embedding of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/embedding/gensim_w2v.py</code> <pre><code>def get_embedding(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the embedding of the text.\n    :param text:\n    :return:\n    \"\"\"\n    embeddings = []\n    if not text:\n        embeddings.append(np.zeros(self.model.vector_size))\n    for word in self.split(str(text)):\n        if word in self.model:\n            embeddings.append(self.model[word])\n        else:\n            embeddings.append(np.zeros(self.model.vector_size))\n    return np.mean(embeddings, axis=0)\n</code></pre>"},{"location":"reference/embedding/huggingface/","title":"Huggingface","text":""},{"location":"reference/embedding/huggingface/#embedding.huggingface.HuggingFaceEmbedding","title":"<code>HuggingFaceEmbedding</code>","text":"<p>               Bases: <code>AbstractEmbeddingModel</code></p> <p>Class for embedding models using HuggingFace.</p> Source code in <code>src/embedding/huggingface.py</code> <pre><code>class HuggingFaceEmbedding(AbstractEmbeddingModel):\n    \"\"\"\n    Class for embedding models using HuggingFace.\n    \"\"\"\n\n    def __init__(self, name, model, split_camel: bool = False):\n        super().__init__(split_camel=split_camel)\n        self._name = f\"{name}\"\n        do_lower_case = True\n        self.model = BertModel.from_pretrained(model)\n        self.tokenizer = BertTokenizer.from_pretrained(\n            model, do_lower_case=do_lower_case\n        )\n\n    def get_embedding(self, text: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the embedding of the text.\n        :param text:\n        :return:\n        \"\"\"\n        if self._split_camel:\n            text = \" \".join(self.split(text))\n        input_ids = torch.tensor(self.tokenizer.encode(text)).unsqueeze(\n            0\n        )  # Batch size 1\n        outputs = self.model(input_ids)\n        last_hidden_states = outputs[\n            0\n        ]  # The last hidden-state is the first element of the output tuple\n        return last_hidden_states.mean(1).detach().numpy()[0]\n</code></pre>"},{"location":"reference/embedding/huggingface/#embedding.huggingface.HuggingFaceEmbedding.get_embedding","title":"<code>get_embedding(text)</code>","text":"<p>Returns the embedding of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/embedding/huggingface.py</code> <pre><code>def get_embedding(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the embedding of the text.\n    :param text:\n    :return:\n    \"\"\"\n    if self._split_camel:\n        text = \" \".join(self.split(text))\n    input_ids = torch.tensor(self.tokenizer.encode(text)).unsqueeze(\n        0\n    )  # Batch size 1\n    outputs = self.model(input_ids)\n    last_hidden_states = outputs[\n        0\n    ]  # The last hidden-state is the first element of the output tuple\n    return last_hidden_states.mean(1).detach().numpy()[0]\n</code></pre>"},{"location":"reference/embedding/huggingface/#embedding.huggingface.SentenceTransformersEmbedding","title":"<code>SentenceTransformersEmbedding</code>","text":"<p>               Bases: <code>AbstractEmbeddingModel</code></p> Source code in <code>src/embedding/huggingface.py</code> <pre><code>class SentenceTransformersEmbedding(AbstractEmbeddingModel):\n    def __init__(self, name, model, device=\"cpu\", split_camel: bool = False):\n        super().__init__(split_camel=split_camel)\n        self._name = f\"{name}\"\n        self.model = SentenceTransformer(model, device=device)\n        self.model.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\n    def get_embedding(self, text: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the embedding of the text.\n        :param text:\n        :return:\n        \"\"\"\n        if self._split_camel:\n            text = \" \".join(self.split(text))\n        embeddings = self.model.encode(text)\n        return embeddings\n</code></pre>"},{"location":"reference/embedding/huggingface/#embedding.huggingface.SentenceTransformersEmbedding.get_embedding","title":"<code>get_embedding(text)</code>","text":"<p>Returns the embedding of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/embedding/huggingface.py</code> <pre><code>def get_embedding(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the embedding of the text.\n    :param text:\n    :return:\n    \"\"\"\n    if self._split_camel:\n        text = \" \".join(self.split(text))\n    embeddings = self.model.encode(text)\n    return embeddings\n</code></pre>"},{"location":"reference/embedding/spacy_bert/","title":"Spacy bert","text":""},{"location":"reference/embedding/spacy_bert/#embedding.spacy_bert.BERTEmbedding","title":"<code>BERTEmbedding</code>","text":"<p>               Bases: <code>AbstractEmbeddingModel</code></p> <p>Class for embedding models using BERT.</p> Source code in <code>src/embedding/spacy_bert.py</code> <pre><code>class BERTEmbedding(AbstractEmbeddingModel):\n    \"\"\"\n    Class for embedding models using BERT.\n    \"\"\"\n\n    def __init__(self, model, split_camel: bool = False):\n        super().__init__(split_camel=split_camel)\n        self._name = f\"{model}\"\n        self.model = spacy.load(model, disable=[\"ner\", \"textcat\", \"parser\"])\n\n    def get_embedding(self, text: str) -&gt; np.ndarray:\n        \"\"\"\n        Returns the embedding of the text.\n        :param text:\n        :return:\n        \"\"\"\n        if self._split_camel:\n            text = \" \".join(self.split(text))\n        return self.model(text).vector\n</code></pre>"},{"location":"reference/embedding/spacy_bert/#embedding.spacy_bert.BERTEmbedding.get_embedding","title":"<code>get_embedding(text)</code>","text":"<p>Returns the embedding of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>ndarray</code> Source code in <code>src/embedding/spacy_bert.py</code> <pre><code>def get_embedding(self, text: str) -&gt; np.ndarray:\n    \"\"\"\n    Returns the embedding of the text.\n    :param text:\n    :return:\n    \"\"\"\n    if self._split_camel:\n        text = \" \".join(self.split(text))\n    return self.model(text).vector\n</code></pre>"},{"location":"reference/ensemble/","title":"Index","text":""},{"location":"reference/ensemble/avg/","title":"Avg","text":""},{"location":"reference/ensemble/avg/#ensemble.avg.AverageEnsemble","title":"<code>AverageEnsemble</code>","text":"<p>               Bases: <code>EnsembleBase</code></p> <p>Ensemble method that averages the annotations.</p> Source code in <code>src/ensemble/avg.py</code> <pre><code>class AverageEnsemble(EnsembleBase):\n    \"\"\"\n    Ensemble method that averages the annotations.\n    \"\"\"\n\n    def run(self, annotations: List[Annotation]) -&gt; Annotation:\n        annotated = np.array([x.distribution for x in annotations if not x.unannotated])\n        if annotated:\n            mean = np.mean(annotated, axis=0)\n            return Annotation(distribution=mean.tolist(), unannotated=0)\n\n        return Annotation(distribution=annotations[0].distribution, unannotated=1)\n</code></pre>"},{"location":"reference/ensemble/cascade/","title":"Cascade","text":""},{"location":"reference/ensemble/cascade/#ensemble.cascade.CascadeEnsemble","title":"<code>CascadeEnsemble</code>","text":"<p>               Bases: <code>EnsembleBase</code></p> <p>Ensemble method that iterates over the annotations and picks the first annotation that is not unannotated.</p> Source code in <code>src/ensemble/cascade.py</code> <pre><code>class CascadeEnsemble(EnsembleBase):\n    \"\"\"\n    Ensemble method that iterates over the annotations and picks the first annotation that is not unannotated.\n    \"\"\"\n\n    def run(self, annotations: List[Annotation]) -&gt; Annotation:\n        for annotation in annotations:\n            if not annotation.unannotated:\n                return Annotation(distribution=annotation.distribution,\n                                  unannotated=0)\n\n        return Annotation(distribution=annotations[0].distribution, unannotated=1)\n</code></pre>"},{"location":"reference/ensemble/ensemble/","title":"Ensemble","text":""},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.EnsembleBase","title":"<code>EnsembleBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for ensemble methods. Ensemble methods are used to combine the annotations of multiple annotators into a single annotation. The ensemble method is called with a list of annotations, where each annotation is a list of probabilities for each label. The ensemble method should return a single annotation, which is a list of probabilities for each label.</p> Source code in <code>src/ensemble/ensemble.py</code> <pre><code>class EnsembleBase(ABC):\n    \"\"\"\n    Base class for ensemble methods. Ensemble methods are used to combine the annotations of multiple annotators into a single annotation.\n    The ensemble method is called with a list of annotations, where each annotation is a list of\n    probabilities for each label.\n    The ensemble method should return a single annotation, which is a list of probabilities for each label.\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def __call__(\n        self, annotations: List[Union[np.array, Annotation]], *args, **kwargs\n    ) -&gt; Annotation:\n        \"\"\"\n        Making the ensemble method callable allows to also define functions as ensemble methods instead of classes. This is useful for ensemble methods that do not have any state.\n        :param annotations:\n        :param args:\n        :param kwargs:\n        :return:\n        \"\"\"\n        annotation: Annotation = self.run(annotations)\n        annotation.distribution = self.normalize(annotation.distribution).tolist()\n        return annotation\n\n    def run(self, annotations: List[Annotation]) -&gt; Annotation:\n        \"\"\"\n        Run the ensemble method. This method should be implemented by subclasses.\n        The ensemble method is called with a list of annotations, where each annotation is a list of probabilities for each label.\n        The ensemble method should return a single Annotation. If the ensemble method was not able to produce a valid annotation, the ensemble method should return the first annotation in the list of annotations.\n        :param annotations:\n        :return:\n        \"\"\"\n        raise NotImplementedError(\"Ensemble method must implement run() method.\")\n\n    @staticmethod\n    def normalize(annotations: np.array) -&gt; np.array:\n        \"\"\"\n        Normalize the annotations. This method is used to bring the ensemble result into probability vectors.\n        :param annotations:\n        :return:\n        \"\"\"\n        return np.array(annotations) / np.linalg.norm(annotations)\n</code></pre>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.EnsembleBase.__call__","title":"<code>__call__(annotations, *args, **kwargs)</code>","text":"<p>Making the ensemble method callable allows to also define functions as ensemble methods instead of classes. This is useful for ensemble methods that do not have any state.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[Union[array, Annotation]]</code> required <code>args</code> <code>()</code> <code>kwargs</code> <code>{}</code> <p>Returns:</p> Type Description <code>Annotation</code> Source code in <code>src/ensemble/ensemble.py</code> <pre><code>def __call__(\n    self, annotations: List[Union[np.array, Annotation]], *args, **kwargs\n) -&gt; Annotation:\n    \"\"\"\n    Making the ensemble method callable allows to also define functions as ensemble methods instead of classes. This is useful for ensemble methods that do not have any state.\n    :param annotations:\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n    annotation: Annotation = self.run(annotations)\n    annotation.distribution = self.normalize(annotation.distribution).tolist()\n    return annotation\n</code></pre>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.EnsembleBase.normalize","title":"<code>normalize(annotations)</code>  <code>staticmethod</code>","text":"<p>Normalize the annotations. This method is used to bring the ensemble result into probability vectors.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>array</code> required <p>Returns:</p> Type Description <code>array</code> Source code in <code>src/ensemble/ensemble.py</code> <pre><code>@staticmethod\ndef normalize(annotations: np.array) -&gt; np.array:\n    \"\"\"\n    Normalize the annotations. This method is used to bring the ensemble result into probability vectors.\n    :param annotations:\n    :return:\n    \"\"\"\n    return np.array(annotations) / np.linalg.norm(annotations)\n</code></pre>"},{"location":"reference/ensemble/ensemble/#ensemble.ensemble.EnsembleBase.run","title":"<code>run(annotations)</code>","text":"<p>Run the ensemble method. This method should be implemented by subclasses. The ensemble method is called with a list of annotations, where each annotation is a list of probabilities for each label. The ensemble method should return a single Annotation. If the ensemble method was not able to produce a valid annotation, the ensemble method should return the first annotation in the list of annotations.</p> <p>Parameters:</p> Name Type Description Default <code>annotations</code> <code>List[Annotation]</code> required <p>Returns:</p> Type Description <code>Annotation</code> Source code in <code>src/ensemble/ensemble.py</code> <pre><code>def run(self, annotations: List[Annotation]) -&gt; Annotation:\n    \"\"\"\n    Run the ensemble method. This method should be implemented by subclasses.\n    The ensemble method is called with a list of annotations, where each annotation is a list of probabilities for each label.\n    The ensemble method should return a single Annotation. If the ensemble method was not able to produce a valid annotation, the ensemble method should return the first annotation in the list of annotations.\n    :param annotations:\n    :return:\n    \"\"\"\n    raise NotImplementedError(\"Ensemble method must implement run() method.\")\n</code></pre>"},{"location":"reference/ensemble/none/","title":"None","text":""},{"location":"reference/ensemble/none/#ensemble.none.NoneEnsemble","title":"<code>NoneEnsemble</code>","text":"<p>               Bases: <code>EnsembleBase</code></p> <p>Ensemble method that does not do anything. This is useful for single annotator experiments.</p> Source code in <code>src/ensemble/none.py</code> <pre><code>class NoneEnsemble(EnsembleBase):\n    \"\"\"\n    Ensemble method that does not do anything. This is useful for single annotator experiments.\n    \"\"\"\n\n    def run(self, annotations: List[Annotation]) -&gt; Annotation:\n        return Annotation(distribution=annotations[0].distribution, unannotated=0)\n</code></pre>"},{"location":"reference/ensemble/voting/","title":"Voting","text":""},{"location":"reference/entity/","title":"Index","text":""},{"location":"reference/entity/analysis/","title":"Analysis","text":""},{"location":"reference/entity/analysis/#entity.analysis.Analysis","title":"<code>Analysis</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class modelling the analysis run. Contains the project info and configs for the annotation.</p> Source code in <code>src/entity/analysis.py</code> <pre><code>class Analysis(BaseModel):\n    \"\"\"\n    Class modelling the analysis run. Contains the project info and configs for the annotation.\n    \"\"\"\n\n    name: str\n    remote: str\n    languages: List[str]\n    config: Optional[Dict[str, str]] = None\n</code></pre>"},{"location":"reference/entity/annotation/","title":"Annotation","text":""},{"location":"reference/entity/annotation/#entity.annotation.Annotation","title":"<code>Annotation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class defining the annotation assigned to a file.</p> Source code in <code>src/entity/annotation.py</code> <pre><code>class Annotation(BaseModel):\n    \"\"\"\n    Class defining the annotation assigned to a file.\n    \"\"\"\n\n    distribution: List[float]\n    unannotated: int\n    raw_annotation: List[float] = None\n</code></pre>"},{"location":"reference/entity/file/","title":"File","text":""},{"location":"reference/entity/file/#entity.file.File","title":"<code>File</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class defining a file. Each file has a path, a language, a content, a list of identifiers and a package.</p> Source code in <code>src/entity/file.py</code> <pre><code>class File(BaseModel):\n    \"\"\"\n    Class defining a file. Each file has a path, a language, a content, a list of identifiers and a package.\n    \"\"\"\n\n    path: str\n    language: str\n    content: Optional[str] = None\n    identifiers: Optional[List[str]] = None\n    package: Optional[str] = None\n    annotation: Optional[Annotation] = None\n    individual_annotations: Optional[dict] = None\n</code></pre>"},{"location":"reference/entity/project/","title":"Project","text":""},{"location":"reference/entity/project/#entity.project.Project","title":"<code>Project</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing a project. Each project has a name, a remote (url), a directory path, a list of languages, a list of versions, a list of keywords, a taxonomy and a list of predicted labels and developer assigned labels.</p> Source code in <code>src/entity/project.py</code> <pre><code>class Project(BaseModel):\n    \"\"\"\n    Class representing a project. Each project has a name, a remote (url), a directory path, a list of languages,\n    a list of versions, a list of keywords, a taxonomy and a list of predicted labels and developer assigned labels.\n    \"\"\"\n\n    name: str\n    cfg: Optional[Dict[str, Any]] = None\n    remote: Optional[str] = None\n    dir_path: Optional[str] = None\n    languages: Optional[List[str]] = None\n    versions: Optional[List[Version]] = []\n    taxonomy: Optional[Dict[str, str]] = None\n    project_annotation: Optional[Dict[str, list]] = {}\n    predicted_labels: Optional[List[str]] = None\n    dev_labels: Optional[List[str]] = None\n</code></pre>"},{"location":"reference/entity/project/#entity.project.Version","title":"<code>Version</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing a version. Each version has a commit id (sha), a commit number describing the position of the commit in the project's history, a commit date and a map of files.</p> Source code in <code>src/entity/project.py</code> <pre><code>class Version(BaseModel):\n    \"\"\"\n    Class representing a version. Each version has a commit id (sha), a commit number describing the position of the\n    commit in the project's history, a commit date and a map of files.\n    \"\"\"\n\n    commit_id: str\n    commit_num: int = None\n    commit_date: datetime = None\n    keywords: Optional[List[str]] = None\n    files: Optional[Dict[str, File]] = None\n    package_annotation: Optional[Dict[str, list]] = {}\n</code></pre>"},{"location":"reference/entity/project/#entity.project.VersionBuilder","title":"<code>VersionBuilder</code>","text":"<p>Class responsible for building a version from a given commit id and a list of languages.</p> Source code in <code>src/entity/project.py</code> <pre><code>class VersionBuilder:\n    \"\"\"\n    Class responsible for building a version from a given commit id and a list of languages.\n    \"\"\"\n\n    def build_version(\n        self, repo_dir: Path | str, languages: List[str], version: Version\n    ) -&gt; Version:\n        files = self.load_files(Path(repo_dir), languages)\n        version.files = files\n        return version\n\n    def load_files(self, repo_dir: Path, languages: List[str]) -&gt; Dict[str, File]:\n        extensions = self.get_languages_ext(languages)\n        all_paths = list(repo_dir.glob(\"**/*\"))\n        filtered_paths = [\n            Path(x) for x in all_paths if x.is_file() and x.suffix.lower() in extensions\n        ]\n        files = {}\n        for path in filtered_paths:\n            rel_path = str(path.relative_to(repo_dir))\n            language = self.language_from_ext(path.suffix, languages)\n            content = self.read_file(path)\n            file = File(path=rel_path, language=language, content=content)\n            files[rel_path] = file\n\n        return files\n\n    @staticmethod\n    def read_file(path: Path) -&gt; str:\n        try:\n            with open(path, \"rt\") as inf:\n                content = \" \".join(inf.readlines())\n        except:\n            with open(path, \"rt\", encoding=\"windows-1252\") as inf:\n                content = \" \".join(inf.readlines())\n\n        return content\n\n    @staticmethod\n    def get_languages_ext(languages):\n        extensions = set()\n        for lang in languages:\n            exts = Extension[lang.lower()].value\n            extensions.update(exts)\n\n        return extensions\n\n    @staticmethod\n    def language_from_ext(extension, languages):\n        for lang in languages:\n            if extension in Extension[lang.lower()].value:\n                return lang\n</code></pre>"},{"location":"reference/entity/taxonomy/","title":"Taxonomy","text":""},{"location":"reference/entity/taxonomy/#entity.taxonomy.KeywordTaxonomy","title":"<code>KeywordTaxonomy</code>","text":"<p>               Bases: <code>TaxonomyBase</code></p> <p>Class defining a taxonomy with keywords and weights for each label.</p> Source code in <code>src/entity/taxonomy.py</code> <pre><code>class KeywordTaxonomy(TaxonomyBase):\n    \"\"\"\n    Class defining a taxonomy with keywords and weights for each label.\n    \"\"\"\n    def __init__(self, path: Union[str, Path], keywords_path: Union[str, Path]):\n        super().__init__(path)\n        self.keywords_path: str = keywords_path\n        self.load()\n\n    def load(self):\n        \"\"\"\n        Loads the taxonomy labels with the keywords and weights\n        :return:\n        \"\"\"\n        with open(self.path, 'rt') as inf:\n            labels = json.load(inf)\n\n        keywords_folders = glob.glob(f\"{self.keywords_path}/*\")\n        keywords = defaultdict(lambda: defaultdict(set))\n        weights = defaultdict(lambda: defaultdict(dict))\n\n        for folder_path in keywords_folders:\n            folder = Path(folder_path).stem\n            keywords_files = [Path(x) for x in glob.glob(f\"{folder_path}/*.csv\")]\n\n            for file in keywords_files:\n                df = pd.read_csv(file)\n                kw_weight = list(zip(df['keyword'], df['tfidf']))\n                keywords[folder][file.stem] = set([kw for kw, _ in kw_weight])\n                for kw, w in kw_weight:\n                    weights[folder][file.stem][kw] = w\n\n        for folder_path in keywords_folders:\n            folder = Path(folder_path).stem\n            for name, idx in labels.items():\n                    label = KeywordLabel(index=idx, name=name, keywords=keywords[folder][name], weights=weights[folder][name])\n                    self.name_to_label[name] = label\n                    self.id_to_label[idx] = label\n</code></pre>"},{"location":"reference/entity/taxonomy/#entity.taxonomy.KeywordTaxonomy.load","title":"<code>load()</code>","text":"<p>Loads the taxonomy labels with the keywords and weights</p> <p>Returns:</p> Type Description Source code in <code>src/entity/taxonomy.py</code> <pre><code>def load(self):\n    \"\"\"\n    Loads the taxonomy labels with the keywords and weights\n    :return:\n    \"\"\"\n    with open(self.path, 'rt') as inf:\n        labels = json.load(inf)\n\n    keywords_folders = glob.glob(f\"{self.keywords_path}/*\")\n    keywords = defaultdict(lambda: defaultdict(set))\n    weights = defaultdict(lambda: defaultdict(dict))\n\n    for folder_path in keywords_folders:\n        folder = Path(folder_path).stem\n        keywords_files = [Path(x) for x in glob.glob(f\"{folder_path}/*.csv\")]\n\n        for file in keywords_files:\n            df = pd.read_csv(file)\n            kw_weight = list(zip(df['keyword'], df['tfidf']))\n            keywords[folder][file.stem] = set([kw for kw, _ in kw_weight])\n            for kw, w in kw_weight:\n                weights[folder][file.stem][kw] = w\n\n    for folder_path in keywords_folders:\n        folder = Path(folder_path).stem\n        for name, idx in labels.items():\n                label = KeywordLabel(index=idx, name=name, keywords=keywords[folder][name], weights=weights[folder][name])\n                self.name_to_label[name] = label\n                self.id_to_label[idx] = label\n</code></pre>"},{"location":"reference/entity/taxonomy/#entity.taxonomy.TaxonomyBase","title":"<code>TaxonomyBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Class defining the taxonomy of the project. Each taxonomy has a path, a mapping between the label name and the label index and a mapping between the label index and the label name.</p> Source code in <code>src/entity/taxonomy.py</code> <pre><code>class TaxonomyBase(ABC):\n    \"\"\"\n    Class defining the taxonomy of the project. Each taxonomy has a path, a mapping between the label name and the label\n    index and a mapping between the label index and the label name.\n    \"\"\"\n    def __init__(self, path: Union[str, Path]):\n        self.path = path\n        self.name_to_label: Dict[str, LabelBase] = {}\n        self.id_to_label: Dict[int: LabelBase] = {}\n        self.n = 0\n\n    @abstractmethod\n    def load(self):\n        pass\n\n    def __getitem__(self, item: str | int):\n        if isinstance(item, str):\n            return self.name_to_label[item]\n        else:\n            return self.id_to_label[item]\n\n    def __len__(self):\n        return len(self.name_to_label)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self) -&gt; LabelBase:\n        if self.n &lt; len(self):\n            label = self.id_to_label[self.n]\n            self.n += 1\n            return label\n        else:\n            self.n = 0\n            raise StopIteration\n\n    def __str__(self):\n        return str([self.name_to_label[x] for x in self.name_to_label])\n</code></pre>"},{"location":"reference/execution/","title":"Index","text":""},{"location":"reference/execution/annotation/","title":"Annotation","text":""},{"location":"reference/execution/execution/","title":"Execution","text":""},{"location":"reference/execution/keyword_extraction/","title":"Keyword extraction","text":""},{"location":"reference/keyword_extraction/","title":"Index","text":""},{"location":"reference/keyword_extraction/keyword_extraction/","title":"Keyword extraction","text":""},{"location":"reference/keyword_extraction/keyword_extraction/#keyword_extraction.keyword_extraction.KeywordExtractionBase","title":"<code>KeywordExtractionBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for keyword extraction</p> Source code in <code>src/keyword_extraction/keyword_extraction.py</code> <pre><code>class KeywordExtractionBase(ABC):\n    \"\"\"\n    Abstract class for keyword extraction\n    \"\"\"\n\n    def __init__(self):\n        self.model = None\n\n    def get_keywords(self, text: str) -&gt; List[str]:\n        pass\n</code></pre>"},{"location":"reference/keyword_extraction/rake/","title":"Rake","text":""},{"location":"reference/keyword_extraction/rake/#keyword_extraction.rake.RAKEKeywordExtraction","title":"<code>RAKEKeywordExtraction</code>","text":"<p>               Bases: <code>KeywordExtractionBase</code></p> <p>Class for keyword extraction using RAKE.</p> Source code in <code>src/keyword_extraction/rake.py</code> <pre><code>class RAKEKeywordExtraction(KeywordExtractionBase):\n    \"\"\"\n    Class for keyword extraction using RAKE.\n    \"\"\"\n\n    def __init__(self, stopwords_path=None, **kwargs):\n        super().__init__()\n        self.model = Rake(stopwords_path)\n        self.kwargs = kwargs\n\n    def get_keywords(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Returns the keywords of the text.\n        :param text:\n        :return:\n        \"\"\"\n        return self.model.run(text, **self.kwargs)\n</code></pre>"},{"location":"reference/keyword_extraction/rake/#keyword_extraction.rake.RAKEKeywordExtraction.get_keywords","title":"<code>get_keywords(text)</code>","text":"<p>Returns the keywords of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>List[str]</code> Source code in <code>src/keyword_extraction/rake.py</code> <pre><code>def get_keywords(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Returns the keywords of the text.\n    :param text:\n    :return:\n    \"\"\"\n    return self.model.run(text, **self.kwargs)\n</code></pre>"},{"location":"reference/keyword_extraction/yake/","title":"Yake","text":""},{"location":"reference/keyword_extraction/yake/#keyword_extraction.yake.YAKEKeywordExtraction","title":"<code>YAKEKeywordExtraction</code>","text":"<p>               Bases: <code>KeywordExtractionBase</code></p> <p>Class for keyword extraction using YAKE.</p> Source code in <code>src/keyword_extraction/yake.py</code> <pre><code>class YAKEKeywordExtraction(KeywordExtractionBase):\n    \"\"\"\n    Class for keyword extraction using YAKE.\n    \"\"\"\n\n    def __init__(self, stopwords_path=None, min_characters=3, **kwargs):\n        super().__init__()\n        self.stopwords = self.load_stopwords(stopwords_path)\n        self.model = yake.KeywordExtractor(stopwords=self.stopwords, **kwargs)\n        self.min_chars = min_characters\n\n    def get_keywords(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Returns the keywords of the text.\n        :param text:\n        :return:\n        \"\"\"\n        return self.model.extract_keywords(text)\n\n    @staticmethod\n    def load_stopwords(path: str) -&gt; List[str]:\n        with open(path, \"rt\") as f:\n            stopwords = f.read().splitlines()\n        return stopwords\n</code></pre>"},{"location":"reference/keyword_extraction/yake/#keyword_extraction.yake.YAKEKeywordExtraction.get_keywords","title":"<code>get_keywords(text)</code>","text":"<p>Returns the keywords of the text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> required <p>Returns:</p> Type Description <code>List[str]</code> Source code in <code>src/keyword_extraction/yake.py</code> <pre><code>def get_keywords(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Returns the keywords of the text.\n    :param text:\n    :return:\n    \"\"\"\n    return self.model.extract_keywords(text)\n</code></pre>"},{"location":"reference/parser/","title":"Index","text":""},{"location":"reference/parser/extensions/","title":"Extensions","text":""},{"location":"reference/parser/parser/","title":"Parser","text":""},{"location":"reference/parser/parser/#parser.parser.ParserBase","title":"<code>ParserBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for a programming language parser.</p> Source code in <code>src/parser/parser.py</code> <pre><code>class ParserBase(ABC):\n    \"\"\"\n    Abstract class for a programming language parser.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        \"\"\"\n        :param library_path: Path to the tree-sitter languages.so file. The file has to contain the\n        language parser. See tree-sitter for more details\n        \"\"\"\n        self.library_path = library_path\n        self.parser = Parser()\n        self.language: Language = None\n        self.identifiers_pattern: str = \"\"\n        self.identifiers_query = None\n        self.keywords = set()\n\n    def parse(self, file: File) -&gt; Tuple[List[str], str]:\n        \"\"\"\n        :param file:\n        :return:\n        \"\"\"\n        code = bytes(file.content, \"utf8\")\n        tree = self.parser.parse(code)\n        identifiers_nodes = self.identifiers_query.captures(tree.root_node)\n        identifiers = self.get_node_text(code, identifiers_nodes)\n        identifiers = [x for x in identifiers if x not in self.keywords]\n        package = self.get_package(file.path, code, tree)\n        return identifiers, package\n\n    @staticmethod\n    def get_node_text(\n        code: bytes, identifiers_nodes: dict[str, list[Node]]\n    ) -&gt; List[str]:\n        identifiers = []\n        for type in identifiers_nodes:\n            for node in identifiers_nodes[type]:\n                token = code[node.start_byte : node.end_byte]\n                identifiers.append(token.decode())\n\n        return identifiers\n\n    def get_package(self, file: str, code: bytes, root: Tree) -&gt; str:\n        package = \".\"\n\n        return package\n\n    def __init_subclass__(cls, lang: str):\n        ParserFactory.register(lang, parser_class=cls)\n</code></pre>"},{"location":"reference/parser/parser/#parser.parser.ParserBase.__init__","title":"<code>__init__(library_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>library_path</code> <code>Path | str</code> <p>Path to the tree-sitter languages.so file. The file has to contain the language parser. See tree-sitter for more details</p> required Source code in <code>src/parser/parser.py</code> <pre><code>def __init__(self, library_path: Path | str):\n    \"\"\"\n    :param library_path: Path to the tree-sitter languages.so file. The file has to contain the\n    language parser. See tree-sitter for more details\n    \"\"\"\n    self.library_path = library_path\n    self.parser = Parser()\n    self.language: Language = None\n    self.identifiers_pattern: str = \"\"\n    self.identifiers_query = None\n    self.keywords = set()\n</code></pre>"},{"location":"reference/parser/parser/#parser.parser.ParserBase.parse","title":"<code>parse(file)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file</code> <code>File</code> required <p>Returns:</p> Type Description <code>Tuple[List[str], str]</code> Source code in <code>src/parser/parser.py</code> <pre><code>def parse(self, file: File) -&gt; Tuple[List[str], str]:\n    \"\"\"\n    :param file:\n    :return:\n    \"\"\"\n    code = bytes(file.content, \"utf8\")\n    tree = self.parser.parse(code)\n    identifiers_nodes = self.identifiers_query.captures(tree.root_node)\n    identifiers = self.get_node_text(code, identifiers_nodes)\n    identifiers = [x for x in identifiers if x not in self.keywords]\n    package = self.get_package(file.path, code, tree)\n    return identifiers, package\n</code></pre>"},{"location":"reference/parser/parser/#parser.parser.ParserFactory","title":"<code>ParserFactory</code>","text":"<p>The factory class for creating parsers</p> Source code in <code>src/parser/parser.py</code> <pre><code>class ParserFactory:\n    \"\"\"The factory class for creating parsers\"\"\"\n\n    registry = {}\n    \"\"\" Internal registry for available parsers \"\"\"\n\n    @classmethod\n    def register(cls, lang: str, parser_class: ParserBase):\n        cls.registry[lang] = parser_class\n\n    @classmethod\n    def create_parser(cls, name: str, library_path: Path | str, **kwargs) -&gt; ParserBase:\n        try:\n            return cls.registry[name](library_path)\n        except KeyError:\n            raise ValueError(f\"Unknown parser : {name}\")\n</code></pre>"},{"location":"reference/parser/parser/#parser.parser.ParserFactory.registry","title":"<code>registry = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal registry for available parsers</p>"},{"location":"reference/parser/languages/","title":"Index","text":""},{"location":"reference/parser/languages/c/","title":"C","text":""},{"location":"reference/parser/languages/c/#parser.languages.c.CParser","title":"<code>CParser</code>","text":"<p>               Bases: <code>ParserBase</code></p> <p>C specific parser. Uses a generic grammar for multiple versions of c. It uses tree_sitter.</p> Source code in <code>src/parser/languages/c.py</code> <pre><code>class CParser(ParserBase, lang=Extension.c.name):\n    \"\"\"\n    C specific parser. Uses a generic grammar for multiple versions of c. It uses tree_sitter.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        super().__init__(library_path)\n        self.language: Language = Language(tsc.language())\n        self.parser: Parser = Parser(self.language)\n        self.identifiers_pattern: str = \"\"\"\n                                        ((identifier) @identifier)\n                                        ((type_identifier) @type)\n                                        \"\"\"\n        self.identifiers_query = self.language.query(self.identifiers_pattern)\n\n        self.keywords = {'malloc'}\n</code></pre>"},{"location":"reference/parser/languages/cpp/","title":"Cpp","text":""},{"location":"reference/parser/languages/cpp/#parser.languages.cpp.CPPParser","title":"<code>CPPParser</code>","text":"<p>               Bases: <code>ParserBase</code></p> <p>CPP specific parser. Uses a generic grammar for multiple versions of CPP. It uses tree_sitter.</p> Source code in <code>src/parser/languages/cpp.py</code> <pre><code>class CPPParser(ParserBase, lang=Extension.cpp.name):\n    \"\"\"\n    CPP specific parser. Uses a generic grammar for multiple versions of CPP. It uses tree_sitter.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        super().__init__(library_path)\n        self.language: Language = Language(tscpp.language())\n        self.parser: Parser = Parser(self.language)\n        # TODO Fix, doesn't work - It doesn't find the namespaced identifiers\n        self.identifiers_pattern: str = \"\"\"\n                                        ((identifier) @identifier)\n                                        ((type_identifier) @type)\n                                        \"\"\"\n        self.identifiers_query = self.language.query(self.identifiers_pattern)\n\n        self.keywords = set()\n</code></pre>"},{"location":"reference/parser/languages/csharp/","title":"Csharp","text":""},{"location":"reference/parser/languages/csharp/#parser.languages.csharp.CSharpParser","title":"<code>CSharpParser</code>","text":"<p>               Bases: <code>ParserBase</code></p> <p>CSharp specific parser. Uses a generic grammar for multiple versions of CSharp. It uses tree_sitter.</p> Source code in <code>src/parser/languages/csharp.py</code> <pre><code>class CSharpParser(ParserBase, lang=Extension.c_sharp.name):\n    \"\"\"\n    CSharp specific parser. Uses a generic grammar for multiple versions of CSharp. It uses tree_sitter.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        super().__init__(library_path)\n        self.language: Language = Language(tscs.language())\n        self.parser: Parser = Parser(self.language)\n        self.identifiers_pattern: str = \"\"\"\n                                        ((identifier) @identifier)\n                                        \"\"\"\n        self.identifiers_query = self.language.query(self.identifiers_pattern)\n\n        self.keywords = set()\n</code></pre>"},{"location":"reference/parser/languages/java/","title":"Java","text":""},{"location":"reference/parser/languages/java/#parser.languages.java.JavaParser","title":"<code>JavaParser</code>","text":"<p>               Bases: <code>ParserBase</code></p> <p>Java specific parser. Uses a generic grammar for multiple versions of java. It uses tree_sitter.</p> Source code in <code>src/parser/languages/java.py</code> <pre><code>class JavaParser(ParserBase, lang=Extension.java.name):\n    \"\"\"\n    Java specific parser. Uses a generic grammar for multiple versions of java. It uses tree_sitter.\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        super().__init__(library_path)\n        self.language: Language = Language(tsjava.language())\n        self.parser: Parser = Parser(self.language)\n        self.identifiers_pattern: str = \"\"\"\n                                        ((identifier) @identifier)\n                                        ((type_identifier) @type)\n                                        \"\"\"\n        self.identifiers_query = self.language.query(self.identifiers_pattern)\n\n        self.keywords = set()\n\n        self.package_pattern = \"\"\"\n                               (package_declaration ((\n                                                    scoped_identifier scope: (\n                                                        scoped_identifier scope: (identifier) name: (identifier)\n                                                    ) \n                                                    name: (identifier)\n                                                    )) @package\n                               )\n                             \"\"\"\n\n        self.package_query = self.language.query(self.package_pattern)\n\n    def get_package(self, file: Path, code: bytes, tree: Tree) -&gt; str:\n        package = self.package_query.captures(tree.root_node)\n        if package:\n            return self.get_node_text(code, package)[0]\n\n        return '.'\n</code></pre>"},{"location":"reference/parser/languages/python/","title":"Python","text":""},{"location":"reference/parser/languages/python/#parser.languages.python.PythonParser","title":"<code>PythonParser</code>","text":"<p>               Bases: <code>ParserBase</code></p> <p>Python specific parser. Uses a generic grammar for multiple versions of python. Uses tree_sitter to get the AST</p> Source code in <code>src/parser/languages/python.py</code> <pre><code>class PythonParser(ParserBase, lang=Extension.python.name):\n    \"\"\"\n    Python specific parser. Uses a generic grammar for multiple versions of python. Uses tree_sitter to get the AST\n    \"\"\"\n\n    def __init__(self, library_path: Path | str):\n        super().__init__(library_path)\n        self.language: Language = Language(tsp.language())\n        self.parser: Parser = Parser(self.language)\n        self.identifiers_pattern: str = \"\"\"\n                                        ((identifier) @identifier)\n                                        \"\"\"\n        self.identifiers_query = self.language.query(self.identifiers_pattern)\n\n        self.keywords = set(keyword.kwlist)  # Use python's built in keyword list\n        self.keywords.update(['self', 'cls'])\n</code></pre>"},{"location":"reference/pipeline/","title":"Index","text":""},{"location":"reference/pipeline/file_annotation/","title":"File annotation","text":""},{"location":"reference/pipeline/identifier_extraction/","title":"Identifier extraction","text":""},{"location":"reference/pipeline/keyword_extraction/","title":"Keyword extraction","text":""},{"location":"reference/pipeline/package_annotation/","title":"Package annotation","text":""},{"location":"reference/pipeline/pipeline/","title":"Pipeline","text":""},{"location":"reference/pipeline/project_annotation/","title":"Project annotation","text":""},{"location":"reference/utils/","title":"Index","text":""},{"location":"reference/utils/instantiators/","title":"Instantiators","text":""},{"location":"reference/utils/instantiators/#utils.instantiators.instantiate_annotators","title":"<code>instantiate_annotators(annotators_cfg, taxonomy)</code>","text":"<p>Instantiates the annotators from the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>annotators_cfg</code> <code>DictConfig</code> required <code>taxonomy</code> required <p>Returns:</p> Type Description <code>List[Annotator]</code> Source code in <code>src/utils/instantiators.py</code> <pre><code>def instantiate_annotators(annotators_cfg: DictConfig, taxonomy) -&gt; List[Annotator]:\n    \"\"\"\n    Instantiates the annotators from the configuration.\n    :param annotators_cfg:\n    :param taxonomy:\n    :return:\n    \"\"\"\n    annotators: List[Annotator] = []\n\n    for name, cb_conf in annotators_cfg.items():\n        if isinstance(cb_conf, DictConfig):\n            logger.info(f\"Instantiating annotator &lt;{name}&gt;\")\n            lf = instantiate(cb_conf['lf'], taxonomy=taxonomy)\n            filtering = instantiate(cb_conf['filtering']) if cb_conf['filtering']['_target_'] else None\n            transformation = instantiate(cb_conf['transformation']) if cb_conf['transformation']['_target_'] else None\n            annotators.append(Annotator(lf, filtering, transformation, name))\n\n    return annotators\n</code></pre>"},{"location":"reference/utils/utils/","title":"Utils","text":""},{"location":"reference/vcs/","title":"Index","text":""},{"location":"reference/vcs/current/","title":"Current","text":""},{"location":"reference/vcs/current/#vcs.current.CurrentVersionStrategy","title":"<code>CurrentVersionStrategy</code>","text":"<p>               Bases: <code>VersionStrategyBase</code></p> <p>Strategy to get the current version of a project from a VCS repository (e.g. git)</p> Source code in <code>src/vcs/current.py</code> <pre><code>class CurrentVersionStrategy(VersionStrategyBase):\n    \"\"\"\n    Strategy to get the current version of a project from a VCS repository (e.g. git)\n    \"\"\"\n\n    def get_versions(self, repository: Repo) -&gt; List[Version]:\n        commit = repository.commit().hexsha\n        commit_num = [\n            i\n            for i, _ in enumerate(repository.iter_commits(\"--all\"))\n            if _.hexsha == commit\n        ][0]\n        return [\n            Version(\n                commit_id=commit,\n                commit_num=commit_num,\n                commit_date=repository.commit().committed_datetime,\n                files=None,\n            )\n        ]\n</code></pre>"},{"location":"reference/vcs/date_range/","title":"Date range","text":""},{"location":"reference/vcs/date_range/#vcs.date_range.DateRangeVersionStrategy","title":"<code>DateRangeVersionStrategy</code>","text":"<p>               Bases: <code>VersionStrategyBase</code></p> <p>Strategy to get the versions of a project from a VCS repository (e.g. git) in a given date range every N days.</p> Source code in <code>src/vcs/date_range.py</code> <pre><code>class DateRangeVersionStrategy(VersionStrategyBase):\n    \"\"\"\n    Strategy to get the versions of a project from a VCS repository (e.g. git) in a given date range every N days.\n    \"\"\"\n\n    def __init__(self, start_date: str, end_date: str, interval_days: int):\n        self.start_date = datetime.strptime(start_date, \"%d/%m/%y\")\n        self.end_date = datetime.strptime(end_date, \"%d/%m/%y\")\n        self.interval_days = interval_days\n\n    def get_versions(self, repository: Repo) -&gt; List[Version]:\n        commits = []\n        for commit in repository.iter_commits(\"--all\"):\n            version_date = datetime.fromtimestamp(commit.authored_date)\n            if self.start_date &lt; version_date &lt; self.end_date:\n                if not commits or not self.interval_days:\n                    commits.append(commit)\n                    continue\n\n                delta = datetime.fromtimestamp(commits[-1].authored_date) - version_date\n                if delta.days &gt;= self.interval_days:\n                    commits.append(commit)\n\n        commits = [\n            Version(\n                commit_id=commit.hexsha,\n                commit_num=len(commits) - i,\n                commit_date=commit.committed_datetime,\n                files=None,\n            )\n            for i, commit in enumerate(commits)\n        ]\n        return commits\n</code></pre>"},{"location":"reference/vcs/first/","title":"First","text":""},{"location":"reference/vcs/first/#vcs.first.FirstVersionStrategy","title":"<code>FirstVersionStrategy</code>","text":"<p>               Bases: <code>VersionStrategyBase</code></p> <p>Strategy to get the first version of a project from a VCS repository (e.g. git)</p> Source code in <code>src/vcs/first.py</code> <pre><code>class FirstVersionStrategy(VersionStrategyBase):\n    \"\"\"\n    Strategy to get the first version of a project from a VCS repository (e.g. git)\n    \"\"\"\n\n    def get_versions(self, repository: Repo) -&gt; List[Version]:\n        commits = list(repository.iter_commits(\"--all\"))\n        commit = commits[0]\n        return [\n            Version(\n                commit_id=commit.hexsha,\n                commit_num=0,\n                commit_date=commit.committed_datetime,\n                files=None,\n            )\n        ]\n</code></pre>"},{"location":"reference/vcs/latest/","title":"Latest","text":""},{"location":"reference/vcs/latest/#vcs.latest.LatestVersionStrategy","title":"<code>LatestVersionStrategy</code>","text":"<p>               Bases: <code>VersionStrategyBase</code></p> <p>Strategy to get the latest version of a project from a VCS repository (e.g. git)</p> Source code in <code>src/vcs/latest.py</code> <pre><code>class LatestVersionStrategy(VersionStrategyBase):\n    \"\"\"\n    Strategy to get the latest version of a project from a VCS repository (e.g. git)\n    \"\"\"\n\n    def get_versions(self, repository: Repo) -&gt; List[Version]:\n        commits = list(repository.iter_commits(\"--all\"))\n        commit = commits[0]\n\n        return [\n            Version(\n                commit_id=commit.hexsha,\n                commit_num=len(commits),\n                commit_date=commit.committed_datetime,\n                files=None,\n            )\n        ]\n</code></pre>"},{"location":"reference/vcs/vcs/","title":"Vcs","text":""},{"location":"reference/vcs/version_strategy/","title":"Version strategy","text":""},{"location":"reference/vcs/version_strategy/#vcs.version_strategy.VersionStrategyBase","title":"<code>VersionStrategyBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Common interface for version strategies. A version strategy is a strategy to get the versions of a project from a VCS repository (e.g. git).</p> Source code in <code>src/vcs/version_strategy.py</code> <pre><code>class VersionStrategyBase(ABC):\n    \"\"\"\n    Common interface for version strategies. A version strategy is a strategy to get the versions of a project from a\n    VCS repository (e.g. git).\n    \"\"\"\n\n    def get_versions(self, repository: Repo) -&gt; List[Version]:\n        pass\n</code></pre>"},{"location":"reference/writer/","title":"Index","text":""},{"location":"reference/writer/file/","title":"File","text":""},{"location":"reference/writer/file/#writer.file.FileWriter","title":"<code>FileWriter</code>","text":"<p>               Bases: <code>WriterBase</code></p> Source code in <code>src/writer/file.py</code> <pre><code>class FileWriter(WriterBase):\n    def __init__(self, out_path: str | Path, exclude: str, indent: None | int = None):\n        \"\"\"\n        :param out_path:\n        :param exclude: String of a dict of fields to exclude\n        :param indent:\n        \"\"\"\n        super().__init__()\n        self.out_path = Path(out_path)\n        self.out_path.mkdir(parents=True, exist_ok=True)\n        self.exclude = json.loads(exclude) if exclude else {}\n        self.indent = indent\n\n    def write(self, project: Project):\n        project_dict = project.model_dump_json(exclude=self.exclude, indent=self.indent)\n\n        out_file = self.out_path.joinpath(f\"{project.name}.json\")\n\n        with open(out_file, \"wt\") as outf:\n            outf.write(project_dict)\n\n    def write_bulk(self, projects: List[Project]):\n        for project in projects:\n            self.write(project)\n</code></pre>"},{"location":"reference/writer/file/#writer.file.FileWriter.__init__","title":"<code>__init__(out_path, exclude, indent=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>out_path</code> <code>str | Path</code> required <code>exclude</code> <code>str</code> <p>String of a dict of fields to exclude</p> required <code>indent</code> <code>None | int</code> <code>None</code> Source code in <code>src/writer/file.py</code> <pre><code>def __init__(self, out_path: str | Path, exclude: str, indent: None | int = None):\n    \"\"\"\n    :param out_path:\n    :param exclude: String of a dict of fields to exclude\n    :param indent:\n    \"\"\"\n    super().__init__()\n    self.out_path = Path(out_path)\n    self.out_path.mkdir(parents=True, exist_ok=True)\n    self.exclude = json.loads(exclude) if exclude else {}\n    self.indent = indent\n</code></pre>"},{"location":"reference/writer/keyword_sql/","title":"Keyword sql","text":""},{"location":"reference/writer/postgres/","title":"Postgres","text":""},{"location":"reference/writer/writer/","title":"Writer","text":""}]}